{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8781b4f-8709-4c9f-9756-6f3098d4cdd3",
   "metadata": {},
   "source": [
    "# UCLA Urban Data Science Final Project\n",
    "## Exploring Micromobility Scooters Around UCLA Campus\n",
    "### By Joanny Leyva and Matthews Cribioli\n",
    "\n",
    "Welcome to our final project notebook for the course Urban Data Science taught at UCLA by Professor Adam-Millard Ball. If you're interested in learning more about course, please follow this [link](https://urbandatascience.its.ucla.edu/) to the course website. \n",
    "\n",
    "The goal of this project was to anylsize the scooter patterns on the UCLA campus. The code as follows takes in data recorded from the Transit App API, determines the amount of scooters surrounding each buidings,  and uses building attributes from Open Street Maps to create machine learnings models as well as anaylze their perforance. This is repeated for the entire dataset, morning, noon, and evening data. We then Cluster the results to find if any particular buildings characteristics and scooter presence numbers are similar enough to find patterns. This code is relatively flexible, can can be recreated for any part of a city with high scooter ridership, especially useful in analyzing patterns on college campuses.\n",
    "\n",
    "\n",
    "Our data consists of 10 minute calls to the API from 9am to 10pm. (If you're interested in gathering your own data from the Transit API, see the file 'Running ucla project loop.py'. You'll have to apply for your own [API key here](https://docs.google.com/forms/d/e/1FAIpQLScZbUsb1G1gRzIkEQo4FuuAbfzQbldTvu6-62j_pSRWPtKZiA/viewform))\n",
    "\n",
    "The comments in the code should guide you through, however if any questions arrise do not hesitate to reach out by email to Matthews Cribioli at matthewscribioli@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0977d55-fae7-465f-b840-c36492be279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "import osmnx as ox\n",
    "import numpy as np\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aef62b7-0a47-4e7b-a830-dde3ea2ce6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the data file names\n",
    "csv_names = glob.glob('Data/Scooters*'.format('csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89a411ae-0c0b-4e30-bcad-7d694eb28b8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m temp_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file)\n\u001b[1;32m      6\u001b[0m temp_df \u001b[38;5;241m=\u001b[39m temp_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetworkName\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetworkId\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtextColor\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubtitle\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstationProperties\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m----> 7\u001b[0m Scooterdata \u001b[38;5;241m=\u001b[39m \u001b[43mScooterdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(temp_df, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5983\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5984\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5985\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5986\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5987\u001b[0m ):\n\u001b[1;32m   5988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "Scooterdata = pd.DataFrame()\n",
    " \n",
    "# append the CSV files together into one large file\n",
    "for file in csv_names:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    temp_df = temp_df[['title', 'id','networkName','networkId','color','textColor','latitude','longitude','type','subtitle','stationProperties','time']]\n",
    "    Scooterdata = Scooterdata.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16ec068-f5d4-4502-8833-262f808431ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'longitude'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Bring in Scooter data, create a geodataframe\u001b[39;00m\n\u001b[1;32m      2\u001b[0m scootergdf \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mGeoDataFrame(\n\u001b[0;32m----> 3\u001b[0m     Scooterdata, geometry\u001b[38;5;241m=\u001b[39mgpd\u001b[38;5;241m.\u001b[39mpoints_from_xy(\u001b[43mScooterdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlongitude\u001b[49m,Scooterdata\u001b[38;5;241m.\u001b[39mlatitude, \n\u001b[1;32m      4\u001b[0m                                           crs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPSG:4326\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      5\u001b[0m scootergdf \u001b[38;5;241m=\u001b[39m scootergdf\u001b[38;5;241m.\u001b[39mto_crs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mESRI:102003\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#keep only scooter data\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5983\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5984\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5985\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5986\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5987\u001b[0m ):\n\u001b[1;32m   5988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'longitude'"
     ]
    }
   ],
   "source": [
    "#Bring in Scooter data, create a geodataframe\n",
    "scootergdf = gpd.GeoDataFrame(\n",
    "    Scooterdata, geometry=gpd.points_from_xy(Scooterdata.longitude,Scooterdata.latitude, \n",
    "                                          crs='EPSG:4326'))\n",
    "scootergdf = scootergdf.to_crs(\"ESRI:102003\")\n",
    "#keep only scooter data\n",
    "filtered_scooterdf = scootergdf[scootergdf['type'] == 'electric-scooter']\n",
    "\n",
    "# Set the geometry column explicitly for the filtered_scooterdf GeoDataFrame\n",
    "filtered_scooterdf = filtered_scooterdf.set_geometry(\"geometry\")\n",
    "\n",
    "#turn the datetime data into just the hour\n",
    "filtered_scooterdf['time'] = filtered_scooterdf['time'].apply(lambda x: x.split(' ')[1].split(':')[0])\n",
    "filtered_scooterdf['time'] = filtered_scooterdf['time'].astype('float32')\n",
    "\n",
    "#create different datasets depending on the time of day\n",
    "filtered_scooterdf_morning = filtered_scooterdf.loc[(filtered_scooterdf['time'] < 11) & (filtered_scooterdf['time'] >= 9)]\n",
    "filtered_scooterdf_noon = filtered_scooterdf.loc[(filtered_scooterdf['time'] < 14) & (filtered_scooterdf['time'] >= 11)]\n",
    "filtered_scooterdf_night = filtered_scooterdf.loc[(filtered_scooterdf['time'] < 22) & (filtered_scooterdf['time'] >= 19)]\n",
    "\n",
    "#make a list of those data sets to iterate over\n",
    "List_of_scooter_datasets = [filtered_scooterdf , filtered_scooterdf_morning , filtered_scooterdf_noon , filtered_scooterdf_night]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c47e3-44b0-4622-92ff-b16bf4a5a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select relavent building data columns\n",
    "address = 'University of California, Los Angeles'\n",
    "osm = ox.geometries_from_address(address,tags={'building':True},dist=1000)\n",
    "\n",
    "columns_to_keep = ['geometry','building','height','name','addr:street','amenity','building:units','ele','sport','leisure']\n",
    "osm = osm[columns_to_keep]\n",
    "\n",
    "#Switch our CRS into a projected version for slightly more accuracy\n",
    "osm = osm.to_crs(\"ESRI:102003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d7457-6b91-4f43-98e2-28dd3b172265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform our scooter analysis for each different dataset\n",
    "i = 0\n",
    "for dataset in List_of_scooter_datasets :\n",
    "    i=i+1\n",
    "    # Perform a spatial join between the datasets\n",
    "    joined = gpd.sjoin(dataset, osm, how=\"inner\", op=\"intersects\")\n",
    "\n",
    "    # Calculate distances using pandas\n",
    "    joined[\"distance\"] = joined.apply(\n",
    "        lambda row: row[\"geometry\"].distance(row[\"geometry\"]), axis=1\n",
    "    )\n",
    "\n",
    "    # Determine proximity threshold \n",
    "    proximity_threshold = 6  # about 20 feet converted to meters\n",
    "\n",
    "    # Count e-scooters near buildings\n",
    "    scooters_near_buildings = joined[joined[\"distance\"] <= proximity_threshold].groupby(\"name\")[\"id\"].count()\n",
    "\n",
    "    # Analyze the counts\n",
    "    print(scooters_near_buildings)\n",
    "\n",
    "    # add total scooter counts back into building data\n",
    "    building_scooter_count = osm.merge(scooters_near_buildings, on = 'name')\n",
    "    building_scooter_count.columns = [*building_scooter_count.columns[:-1], 'Scooter_Count']\n",
    "\n",
    "    # add dummy and other variables for atributes we might find important\n",
    "    building_scooter_count['is_parking_lot'] = building_scooter_count['amenity'].apply(\n",
    "                            lambda x: True if x=='parking' else False)\n",
    "    building_scooter_count['is_sport'] = building_scooter_count['leisure'].apply(\n",
    "                            lambda x: True if x=='sports_centre' or x=='sports_hall' or x=='stadium' or x=='fitness_centre' else False)\n",
    "    building_scooter_count['is_medical'] = building_scooter_count['amenity'].apply(\n",
    "                            lambda x: True if x=='clinic' or x=='hospital' else False)\n",
    "    building_scooter_count['is_library'] = building_scooter_count['amenity'].apply(\n",
    "                            lambda x: True if x=='library' else False)\n",
    "    building_scooter_count['is_dorm'] = building_scooter_count['building'].apply(\n",
    "                            lambda x: True if x=='dormitory' else False)\n",
    "    building_scooter_count[\"lat\"] = building_scooter_count.centroid.x\n",
    "    building_scooter_count[\"long\"] = building_scooter_count.centroid.y\n",
    "    building_scooter_count[\"area\"] = building_scooter_count['geometry'].area\n",
    "\n",
    "    building_scooter_count[['height','building:units','ele']] = building_scooter_count[['height','building:units','ele']].fillna(-999)\n",
    "\n",
    "    if i == 1:\n",
    "        building_scooter_count_total = building_scooter_count\n",
    "    elif i ==2:\n",
    "        building_scooter_count_morning = building_scooter_count\n",
    "    elif i ==3:\n",
    "        building_scooter_count_noon = building_scooter_count\n",
    "    elif i ==4:\n",
    "        building_scooter_count_night = building_scooter_count\n",
    "    else:\n",
    "        print('Something messed up, check the code')\n",
    "        \n",
    "building_scooter_datasets = [building_scooter_count_total, building_scooter_count_morning, building_scooter_count_noon, building_scooter_count_night]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08de39-f266-4f3d-947d-5985b20a4351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train our Random Forest model to predict number of scooters\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import contextily  as ctx\n",
    "\n",
    "xvars = (['is_parking_lot','is_sport','is_medical','is_library','is_dorm','lat','long','height','building:units','ele','area'])\n",
    "yvar = 'Scooter_Count'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da65cd2c-ee53-4298-8023-7e269b1a2edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "for building_scooter_dataset in building_scooter_datasets :\n",
    "    j=j+1\n",
    "    if j == 1:\n",
    "        data_name = 'All Data'\n",
    "    elif j ==2:\n",
    "        data_name = 'Morning Data'\n",
    "    elif j ==3:\n",
    "        data_name = 'Noon Data'\n",
    "    elif j ==4:\n",
    "        data_name = 'Night Data'\n",
    "    else:\n",
    "        print('Something messed up, check the code')\n",
    "        \n",
    "\n",
    "    df_to_fit = building_scooter_dataset[xvars+[yvar]]#.dropna()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_to_fit[xvars], df_to_fit[yvar], test_size = 0.35, random_state = 2)\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators = 200, random_state = 1, n_jobs=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    #How good is our prediction?\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    print('Results of {}. Predicted Mean Scooters: {:.4f}. Actual Mean Scooters: {:.4f}'.format( data_name,\n",
    "        y_pred.mean(), y_test.mean()))\n",
    "    \n",
    "    #What atributes are most important in predicting scooter counts?\n",
    "\n",
    "    importances = rf.feature_importances_\n",
    "\n",
    "    forest_importances = pd.Series(importances, index=X_train.columns)\n",
    "\n",
    "    std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "    forest_importances.sort_values(inplace=True, ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    sns.barplot(x=forest_importances.values, y=forest_importances.index, ax=ax)\n",
    "    ax.set_title(\"Feature importances for {}\".format(data_name))\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    \n",
    "    #lets take all our predicted scooter counts and add them to our building data to plot and see where our model is most accurate\n",
    "    all_predict = rf.predict(pd.concat([X_test,X_train]))\n",
    "    predictions = pd.DataFrame(all_predict, \n",
    "                               columns = ['Predicted_Scooters'])\n",
    "    building_p_scooter_count = building_scooter_count.join(predictions)\n",
    "\n",
    "    #Find the percent error for predicted scooter amounts and plot\n",
    "    building_p_scooter_count['Percent_Error'] = ((building_p_scooter_count['Predicted_Scooters']-building_p_scooter_count['Scooter_Count'])/building_p_scooter_count['Scooter_Count'])*100\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    building_p_scooter_count.plot(column='Percent_Error', ax=ax, cmap = 'coolwarm', legend = True, vmax = 100, vmin= -100, edgecolor='black', linewidth = .5)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"Percent Error of Predicted vs Actual Scooter \\n for {}\".format(data_name))\n",
    "    plt.draw()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    building_p_scooter_count.plot(column='Predicted_Scooters', ax=ax, cmap = 'Blues', legend = True, vmin= 0, vmax = 500, edgecolor='black', linewidth = .5)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"Predicted Number of Scooters by Building \\n for {}\".format(data_name))\n",
    "    plt.draw()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee82af-2fc2-4a3c-9799-71010efe73f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What can clustering reveal to us? Are there certain atributes of buildings that lead to similar scooter counts?\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Start by scaling our non-geographic and non-dummy variables,\n",
    "#keeping only the variables which had any importance in the machine learning models.\n",
    "\n",
    "\n",
    "cols_to_scale = ['lat','long','height','ele','area','Scooter_Count']\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(building_scooter_count_total[cols_to_scale])\n",
    "\n",
    "building_scooter_count_total_scaled = pd.DataFrame(scaler.transform(building_scooter_count_total[cols_to_scale]),columns=cols_to_scale)\n",
    "\n",
    "building_scooter_count_total_to_cluster = building_scooter_count_total_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea4ea8-0199-4e06-ad15-da4e0a05bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this dataset, it is a bit difficult to determine how many clusters would make sense to use\n",
    "#We use the 'elblow method' to determine the optimal amount of clusters to use to tell use the most amount\n",
    "#of info with the least amount of clusters. I'm not an expert on Elbow Curves, and they were not covered in the course, so I've borrowed some code to assist us.\n",
    "#code from https://stackoverflow.com/questions/41540751/sklearn-kmeans-equivalent-of-elbow-method \n",
    "distorsion = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(building_scooter_count_total_to_cluster)\n",
    "    distorsion.append(kmeans.inertia_)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(range(2, 10), distorsion)\n",
    "plt.grid(True)\n",
    "plt.title('Building Scooter Cluster Elbow curve')\n",
    "print('From this graph, our elbow or point of dimishining returns happens at around 7 clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d978ef-535d-43fa-bc48-957b9d6652ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code creates our 6 clusters, and assigns each building into one.\n",
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(building_scooter_count_total_to_cluster)\n",
    "building_scooter_count_total_to_cluster['cluster_id'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ffed7-726d-4e36-b3d6-5879c99347d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use a radar plot to find the common attributes of each plot to see if there are common patterns\n",
    "# code from https://matplotlib.org/stable/gallery/specialty_plots/radar_chart.html\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, RegularPolygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from matplotlib.projections import register_projection\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D\n",
    "\n",
    "\n",
    "def radar_factory(num_vars, frame='circle'):\n",
    "    \"\"\"\n",
    "    Create a radar chart with `num_vars` axes.\n",
    "\n",
    "    This function creates a RadarAxes projection and registers it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_vars : int\n",
    "        Number of variables for radar chart.\n",
    "    frame : {'circle', 'polygon'}\n",
    "        Shape of frame surrounding axes.\n",
    "\n",
    "    \"\"\"\n",
    "    # calculate evenly-spaced axis angles\n",
    "    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
    "\n",
    "    class RadarAxes(PolarAxes):\n",
    "\n",
    "        name = 'radar'\n",
    "        # use 1 line segment to connect specified points\n",
    "        RESOLUTION = 1\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            # rotate plot such that the first axis is at the top\n",
    "            self.set_theta_zero_location('N')\n",
    "\n",
    "        def fill(self, *args, closed=True, **kwargs):\n",
    "            \"\"\"Override fill so that line is closed by default\"\"\"\n",
    "            return super().fill(closed=closed, *args, **kwargs)\n",
    "\n",
    "        def plot(self, *args, **kwargs):\n",
    "            \"\"\"Override plot so that line is closed by default\"\"\"\n",
    "            lines = super().plot(*args, **kwargs)\n",
    "            for line in lines:\n",
    "                self._close_line(line)\n",
    "\n",
    "        def _close_line(self, line):\n",
    "            x, y = line.get_data()\n",
    "            # FIXME: markers at x[0], y[0] get doubled-up\n",
    "            if x[0] != x[-1]:\n",
    "                x = np.append(x, x[0])\n",
    "                y = np.append(y, y[0])\n",
    "                line.set_data(x, y)\n",
    "\n",
    "        def set_varlabels(self, labels):\n",
    "            self.set_thetagrids(np.degrees(theta), labels)\n",
    "\n",
    "        def _gen_axes_patch(self):\n",
    "            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n",
    "            # in axes coordinates.\n",
    "            if frame == 'circle':\n",
    "                return Circle((0.5, 0.5), 0.5)\n",
    "            elif frame == 'polygon':\n",
    "                return RegularPolygon((0.5, 0.5), num_vars,\n",
    "                                      radius=.5, edgecolor=\"k\")\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "        def _gen_axes_spines(self):\n",
    "            if frame == 'circle':\n",
    "                return super()._gen_axes_spines()\n",
    "            elif frame == 'polygon':\n",
    "                # spine_type must be 'left'/'right'/'top'/'bottom'/'circle'.\n",
    "                spine = Spine(axes=self,\n",
    "                              spine_type='circle',\n",
    "                              path=Path.unit_regular_polygon(num_vars))\n",
    "                # unit_regular_polygon gives a polygon of radius 1 centered at\n",
    "                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n",
    "                # 0.5) in axes coordinates.\n",
    "                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n",
    "                                    + self.transAxes)\n",
    "                return {'polar': spine}\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "    register_projection(RadarAxes)\n",
    "    return theta\n",
    "\n",
    "def radar_plot(kmeans, df_scaled):\n",
    "    N  = kmeans.cluster_centers_.shape[1]  # number of columns / variables\n",
    "    k = kmeans.n_clusters\n",
    "    theta = radar_factory(N, frame='polygon')\n",
    "    data = kmeans.cluster_centers_.T\n",
    "    spoke_labels = [col for col in df_scaled.columns if col!='cluster_id']\n",
    "    fig, ax = plt.subplots(figsize=(9, 9),\n",
    "                                subplot_kw=dict(projection='radar'))\n",
    "    fig.subplots_adjust(wspace=0.25, hspace=0.20, top=0.85, bottom=0.05)\n",
    "\n",
    "    ax.plot(theta, data) #, color=color)\n",
    "    ax.set_varlabels(spoke_labels)\n",
    "\n",
    "    # add legend relative to top-left plot\n",
    "    labels = ['Cluster {}'.format(kk) for kk in range(k)]\n",
    "    ax.legend(labels, loc=(0.9, .95),\n",
    "                                labelspacing=0.1, fontsize='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11174ca9-7b92-469d-a18b-e7bb8daa5e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting out Radar plot\n",
    "radar_plot(kmeans,building_scooter_count_total_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d08610-26b8-4167-99b3-96f2d5993a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "### REPEAT FOR BUS LOCATIONS? ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0bcce-e939-43ae-961c-0a1dcf8d5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(building_scooter_count_total_to_cluster.groupby('cluster_id').size())\n",
    "print(building_scooter_count_total_to_cluster.groupby('cluster_id').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee5d4a-c12d-4dcb-bbae-99c871c2e565",
   "metadata": {},
   "source": [
    "From our clustering, we have a couple interesting patterns which emerge. \n",
    "\n",
    "* Cluster 0 have are very low buildings, both in hieght and in elevation with about standard scooter counts.\n",
    "* Cluster 1 are buildings in the south east of campus with slightly lower scooters counts.\n",
    "* Cluster 2 are buildings on the south west of campus, with slightly lower scooter counts than cluster 2.\n",
    "* Cluster 3 had our large surface area buildings, which have slightly higher scooter counts than the median building.\n",
    "* Cluster 4 is made up of high scooter count buildings, they are found in the north-west part of campus and are lower in height and elevtion than the median building.\n",
    "* Cluster 5 are mostly buildings on the north side of campus, a bit bigger and higher up in elevation aiwth about median scooter counts. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
